#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Multi-chain vault analysis + safe JSON export.

Features:
- Performs lifetime metric analysis for all available chains.
- Filters and formats results for the top-performing vaults.
- Safely exports to JSON with NaN/Inf -> null sanitization.
- Normalizes column keys into snake_case.
- Uses column-wise .map(parse_value) to comply with modern pandas.
- Uses allow_nan=False to guarantee strict JSON validity.
"""

import os
import re
import json
import math
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path

# Import core TradingStrategy / eth_defi modules
from eth_defi.vault.base import VaultSpec  # noqa: F401
from eth_defi.vault.vaultdb import VaultDatabase
from eth_defi.chain import get_chain_name
from eth_defi.research.vault_metrics import (
    calculate_lifetime_metrics,
    clean_lifetime_metrics,
    format_lifetime_table,
)

# --------------------------------------------------------------------
# Configuration via environment variables
# --------------------------------------------------------------------
MONTHS = int(os.getenv("MONTHS", "3"))  # Time window in months
EVENT_THRESHOLD = int(os.getenv("EVENT_THRESHOLD", "5"))  # Min event count
MAX_ANNUALISED_RETURN = float(os.getenv("MAX_ANNUALISED_RETURN", "0.5"))  # Cap annualized return at 50%
MIN_TVL = float(os.getenv("MIN_TVL", "50000"))  # Minimum TVL filter
TOP_PER_CHAIN = int(os.getenv("TOP_PER_CHAIN", "30"))  # Top N vaults per chain
OUTPUT_JSON = os.getenv("OUTPUT_JSON", "/root/top_vaults_analysis.json")
DATA_DIR = Path(os.getenv("DATA_DIR", "~/.tradingstrategy/vaults")).expanduser()
PARQUET_FILE = DATA_DIR / "cleaned-vault-prices-1h.parquet"


# --------------------------------------------------------------------
# Helper functions for JSON export
# --------------------------------------------------------------------
def normalize_key(col_name: str) -> str:
    """Convert column headers to normalized snake_case keys."""
    col_name = col_name.strip().lower()
    col_name = re.sub(r"[^a-z0-9]+", "_", col_name)  # Replace spaces and special chars with underscores
    col_name = re.sub(r"_+", "_", col_name)  # Collapse multiple underscores
    return col_name.strip("_")


def parse_value(val):
    """Safely convert cell values to JSON-compliant Python types."""
    # Handle None / NaN / pandas NA
    if val is None:
        return None
    if isinstance(val, float) and math.isnan(val):
        return None
    if pd.isna(val):
        return None

    # Convert pandas.Timestamp or datetime to ISO string
    if isinstance(val, (pd.Timestamp, datetime)):
        return val.isoformat()

    # Handle numeric types (int/float/numpy)
    if isinstance(val, (int, float, np.integer, np.floating)):
        try:
            f = float(val)
        except Exception:
            return None
        if math.isnan(f) or math.isinf(f):
            return None
        # Cast integers back to int when possible
        if isinstance(val, (int, np.integer)) or (f.is_integer() and not math.isinf(f)):
            try:
                return int(f)
            except Exception:
                return f
        return f

    # Handle strings (including %, commas, and "unknown")
    if isinstance(val, str):
        v = val.strip()
        if v == "" or v.lower() == "unknown":
            return None
        # Convert percent strings like "12%" -> 0.12
        if v.endswith("%"):
            try:
                return float(v[:-1]) / 100.0
            except Exception:
                return None
        # Remove thousand separators like "1,234"
        if "," in v:
            try:
                return float(v.replace(",", ""))
            except Exception:
                pass
        # Try to convert numeric string to float
        try:
            return float(v)
        except Exception:
            return v

    # Fallback: return the value as-is
    return val


def sanitize(o):
    """
    Recursively replace NaN/Inf with None for any nested dict/list/scalar.
    Ensures json.dump(..., allow_nan=False) will not raise ValueError.
    """
    if isinstance(o, float):
        if math.isnan(o) or math.isinf(o):
            return None
        return o
    if isinstance(o, (np.floating,)):
        f = float(o)
        return None if math.isnan(f) or math.isinf(f) else f
    if isinstance(o, (int, np.integer)):
        return int(o)
    if isinstance(o, dict):
        return {k: sanitize(v) for k, v in o.items()}
    if isinstance(o, list):
        return [sanitize(v) for v in o]
    return o


# --------------------------------------------------------------------
# Step 2: Load database and parquet price data
# --------------------------------------------------------------------
data_folder = DATA_DIR
vault_db = VaultDatabase.read()
cleaned_data_parquet_file = PARQUET_FILE
prices_df = pd.read_parquet(cleaned_data_parquet_file)

print(f"We have {len(vault_db):,} vaults in the database and {len(prices_df):,} price rows.")

# --------------------------------------------------------------------
# Step 3: Filter data for the last N months
# --------------------------------------------------------------------
last_sample_at = prices_df.index[-1]  # Latest timestamp
three_months_ago = last_sample_at - pd.DateOffset(months=MONTHS)
PERIOD = [three_months_ago, last_sample_at]

mask = (prices_df.index >= PERIOD[0]) & (prices_df.index <= PERIOD[1])
prices_df = prices_df[mask]
print(f"✅ Trimmed to {len(prices_df):,} rows from {PERIOD[0]} to {PERIOD[1]}")

# --------------------------------------------------------------------
# Step 4: Examine per-chain data availability
# --------------------------------------------------------------------
chain_ids = sorted(prices_df["chain"].unique())
for chain_id in chain_ids:
    chain_name = get_chain_name(chain_id)
    print(f"\n🔍 Examining chain {chain_name} ({chain_id})")
    chain_prices_df = prices_df[(prices_df["chain"] == chain_id) & (prices_df.index >= PERIOD[0]) & (prices_df.index <= PERIOD[1])]
    print(f"📈 Rows: {len(chain_prices_df):,} for chain {chain_name}")
    if not chain_prices_df.empty:
        print(chain_prices_df.head(1))
    else:
        print("⚠️ No data available for this chain in selected period.")

# --------------------------------------------------------------------
# Step 5: Tally vault and price counts per chain
# --------------------------------------------------------------------
for selected_chain_id in chain_ids:
    chain_name = get_chain_name(selected_chain_id)
    vault_db_filtered = {spec: vault for spec, vault in vault_db.items() if spec.chain_id == selected_chain_id}
    vault_df = prices_df[prices_df["chain"] == selected_chain_id]
    print(f"Chain {chain_name}: {len(vault_db_filtered):,} vaults, {len(vault_df):,} price rows.")

# --------------------------------------------------------------------
# Step 6: Calculate and clean lifetime metrics per chain
# --------------------------------------------------------------------
combined_lifetime_dfs = []

for selected_chain_id in chain_ids:
    chain_name = get_chain_name(selected_chain_id)
    print(f"\n📊 Calculating lifetime metrics for {chain_name} ({selected_chain_id})")

    # Filter vaults and prices for this chain
    vault_db_filtered = {spec: vault for spec, vault in vault_db.items() if spec.chain_id == selected_chain_id}
    vault_df = prices_df[prices_df["chain"] == selected_chain_id]

    if not vault_db_filtered or vault_df.empty:
        print("⚠️ No vaults or price data found for this chain. Skipping...")
        continue

    # Compute raw metrics
    lifetime_data_df = calculate_lifetime_metrics(vault_df, vault_db_filtered)

    # Clean and cap unrealistic metrics
    print(f"🧹 Cleaning metrics for {len(lifetime_data_df):,} vaults on {chain_name}")
    lifetime_data_df = clean_lifetime_metrics(
        lifetime_data_df,
        max_annualised_return=MAX_ANNUALISED_RETURN,
    )

    # Filter out vaults with too few events
    original_count = len(lifetime_data_df)
    lifetime_data_df = lifetime_data_df[lifetime_data_df["event_count"] >= EVENT_THRESHOLD]
    print(f"✅ Filtered event count >= {EVENT_THRESHOLD}: {len(lifetime_data_df):,} vaults (removed {original_count - len(lifetime_data_df):,})")

    # Tag with chain ID and append to combined list
    lifetime_data_df["chain"] = selected_chain_id
    combined_lifetime_dfs.append(lifetime_data_df)

# Combine results from all chains
if combined_lifetime_dfs:
    all_lifetime_df = pd.concat(combined_lifetime_dfs)
    all_lifetime_df = all_lifetime_df.sort_values("one_month_cagr", ascending=False)
    print(f"\n✅ Final metrics table for all chains: {len(all_lifetime_df):,} vaults total")
    print(all_lifetime_df.head(5))
else:
    print("❌ No metrics were calculated. Check input data.")
    all_lifetime_df = pd.DataFrame()

# --------------------------------------------------------------------
# Step 7: Filter by TVL and format output table
# --------------------------------------------------------------------
if not all_lifetime_df.empty:
    min_tvl = MIN_TVL
    filtered_df = all_lifetime_df[all_lifetime_df["current_nav"] >= min_tvl]
    print(f"\n✅ Vaults filtered by min TVL ${int(min_tvl):,}: {len(filtered_df):,} remaining.")

    # Select top N vaults per chain by 1M CAGR
    top_vaults_per_chain = filtered_df.sort_values("one_month_cagr", ascending=False).groupby("chain", group_keys=False).head(TOP_PER_CHAIN)

    # Format output table for readability
    formatted_df = format_lifetime_table(
        top_vaults_per_chain,
        add_index=True,
        add_address=True,
    )

    # Log short summary for each chain
    for chain_id_display in formatted_df["Chain"].unique():
        chain_df = formatted_df[formatted_df["Chain"] == chain_id_display]
        print(f"\n🔗 Chain {chain_id_display}: Top {len(chain_df)} vaults")
        print(", ".join(chain_df.head(5)["Name"]))
else:
    print("Skipping TVL filtering as no metrics were calculated.")
    formatted_df = pd.DataFrame()

# --------------------------------------------------------------------
## --- Cell 8: Safe export to JSON (sorted by chain + 1M return ann.) ---
if not formatted_df.empty:
    SORT_COLUMN = os.getenv("SORT_COLUMN", "1M return ann.")  # Default sort column
    sort_col_norm = normalize_key(SORT_COLUMN)  # Normalize to match column names, e.g. "1M return ann." -> "1m_return_ann"

    # 1️⃣ Replace ±Inf -> NA, then NA -> None
    df = formatted_df.copy()
    df = df.replace([np.inf, -np.inf], pd.NA)
    df = df.where(pd.notna(df), None)

    # 2️⃣ Normalize column names
    df.columns = [normalize_key(c) for c in df.columns]

    # 3️⃣ Parse each column to JSON-safe values
    for c in df.columns:
        df[c] = df[c].map(parse_value)

    # 4️⃣ Sort by chain (asc) then by chosen metric (desc)
    sort_keys, asc = [], []
    if "chain" in df.columns:
        sort_keys.append("chain")
        asc.append(True)
    if sort_col_norm in df.columns:
        sort_keys.append(sort_col_norm)
        asc.append(False)
    else:
        print(f"⚠️ Sort column '{SORT_COLUMN}' (->{sort_col_norm}) not found; sorting by chain only.")

    if sort_keys:
        df = df.sort_values(by=sort_keys, ascending=asc)

    # 5️⃣ Convert DataFrame → list of dicts
    vaults = df.to_dict(orient="records")

    # 6️⃣ Add metadata and deep sanitize
    output_data = {
        "generated_at": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
        "vaults": sanitize(vaults),
    }

    # 7️⃣ Write to JSON file (strict mode)
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False, allow_nan=False)

    print(f"✅ Exported {len(vaults):,} vaults sorted by {sort_keys} → {OUTPUT_JSON}")
else:
    print("Skipping JSON export as there is no formatted data.")
